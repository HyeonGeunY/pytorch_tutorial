{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff2d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56837b9",
   "metadata": {},
   "source": [
    "1. 파이토치 제공 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c545b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca59e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6fc69a52984776b3c4fb8bf0aa6387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transf) # Transform 에는 PIL 형태여야 한다.\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ce416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a542029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddeed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader) # iterator를 만듦\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aed08eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde8aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f29aa",
   "metadata": {},
   "source": [
    "2. 같은 클래스 별 폴더 이미지 데이터 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e357a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: './class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3712/3088744982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ./class/tiger ./class/lion 처럼 폴더 별로 정리가 잘 되어있는 경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtransf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     ):\n\u001b[1;32m--> 226\u001b[1;33m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[0;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: './class'"
     ]
    }
   ],
   "source": [
    "# ./class/tiger ./class/lion 처럼 폴더 별로 정리가 잘 되어있는 경우\n",
    "transf = tr.Compose([tr.Resize(16), tr.ToTensor()])\n",
    "trainset = torchvision.datasets.ImageFolder(root='./class', transform=transf)\n",
    "trainloader = DataLoader(trainset, batch_size=10, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c65005",
   "metadata": {},
   "source": [
    "개인 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75054a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.random.randint(256, size=(20,32,32,3))\n",
    "train_labels = np.random.randint(2, size=(20,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a09491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)(\n",
    "        self.x_data = self.x_data.permute(0, 3, 1, 2)\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fe4940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorData(train_images, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc16672b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc107bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c6354f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e04b451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 기억해둘 양식\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# transform 모듈은 기본적으로 PIL input이어야 한다.\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2,0,1)\n",
    "        \n",
    "        return inputs, torch.LongTensor(labels)\n",
    "\n",
    "class LinearTensor:\n",
    "    \n",
    "    def __init__(self, slope=1, bias=0):\n",
    "        self.slope = slope\n",
    "        self.bias = bias\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = self.slope*inputs + self.bias\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c0d5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = tr.Compose([ToTensor(), LinearTensor(2,5)])\n",
    "ds1 = MyDataset(train_images, train_labels, transform=trans)\n",
    "train_loader1 = DataLoader(ds1, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1de86197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = ds1[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75c7e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter1 = iter(train_loader1)\n",
    "images1, label1 = dataiter1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fd3aa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[271., 503., 437.,  ...,  45., 461., 367.],\n",
       "         [ 43., 275., 285.,  ..., 123., 421., 419.],\n",
       "         [ 99., 177., 399.,  ..., 269., 283., 451.],\n",
       "         ...,\n",
       "         [399., 419., 163.,  ...,  73., 207.,  47.],\n",
       "         [319., 405., 339.,  ..., 169., 473., 299.],\n",
       "         [401.,  17., 171.,  ...,  11., 331.,  57.]],\n",
       "\n",
       "        [[467., 407., 455.,  ..., 223., 507.,  31.],\n",
       "         [151., 203., 133.,  ..., 229.,  47., 249.],\n",
       "         [499., 249., 147.,  ..., 379., 381., 229.],\n",
       "         ...,\n",
       "         [105., 321., 423.,  ..., 113., 173.,  69.],\n",
       "         [129., 343.,  69.,  ..., 407., 217.,  41.],\n",
       "         [233., 481., 147.,  ...,  45., 435.,  47.]],\n",
       "\n",
       "        [[347., 345.,  83.,  ..., 313., 317., 367.],\n",
       "         [ 87., 493., 271.,  ..., 321., 251., 121.],\n",
       "         [185., 219., 339.,  ...,  17., 259., 259.],\n",
       "         ...,\n",
       "         [377., 117., 309.,  ..., 349.,  99., 435.],\n",
       "         [463., 403., 295.,  ..., 453., 471., 429.],\n",
       "         [ 17., 115., 457.,  ..., 409., 111., 387.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e23d903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 기억해둘 양식\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# transform 모듈은 기본적으로 PIL input이어야 한다. transform 모듈을 사용하고 싶다면 \n",
    "# tf.Compose에 tf.ToPILImage()를 처음에 포함시키기\n",
    "\n",
    "class MyTransform:\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2,0,1)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        transf = tr.Compose([tr.ToPILImage(), tr.ToTensor(), tr.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # tr.Resize(128), \n",
    "        final_output = transf(inputs)\n",
    "        \n",
    "        return final_output, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46baebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = MyDataset(train_images, train_labels, transform=MyTransform())\n",
    "train_loader2 = DataLoader(ds2, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0bf7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = ds2[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20d5d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter2 = iter(train_loader2)\n",
    "images2, label2 = dataiter2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d7ec97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f787568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[176,  47,  89,  ..., 197, 212,  11],\n",
       "         [145,  61,  47,  ..., 167,  13, 134],\n",
       "         [ 89,  63,   3,  ...,  21, 182, 132],\n",
       "         ...,\n",
       "         [236, 140, 191,  ..., 131, 132, 111],\n",
       "         [ 42, 236,   6,  ..., 217, 204, 205],\n",
       "         [234, 154,  15,  ...,  36,  66, 156]],\n",
       "\n",
       "        [[190,  67, 137,  ..., 175, 138,   7],\n",
       "         [248, 119, 252,  ..., 153, 132, 199],\n",
       "         [  3, 116, 119,  ..., 130, 227,  47],\n",
       "         ...,\n",
       "         [ 12, 209,  48,  ..., 208, 252,  55],\n",
       "         [170, 121, 161,  ...,  21, 250,  97],\n",
       "         [252,  73, 176,  ...,  42, 199,  89]],\n",
       "\n",
       "        [[ 45, 129, 206,  ..., 145,  93,  79],\n",
       "         [  4,  37,  39,  ..., 231,  11,   5],\n",
       "         [ 56, 126, 185,  ...,  40, 128, 214],\n",
       "         ...,\n",
       "         [129,  24, 209,  ...,  94,   5,  38],\n",
       "         [ 68, 251, 162,  ..., 137, 151,  22],\n",
       "         [178, 225, 137,  ...,  49, 157,  27]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(train_images).permute(0,3,1,2)[0]\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
